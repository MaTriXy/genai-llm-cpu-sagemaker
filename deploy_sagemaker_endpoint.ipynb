{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This creates the IAM role for SageMaker  with the appropriate permissions. Make sure you have the bucket created first.\n",
    "You normally need to run it only once when setting things up\n",
    "\"\"\"\n",
    "\n",
    "import boto3\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "\n",
    "bucket_name = 'alexvt-adx-emr-eks'  # replace with your bucket name\n",
    "role_name = 'SageMakerExecutionRole'  # replace with your preferred role name\n",
    "\n",
    "assume_role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"sagemaker.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "create_role_response = iam.create_role(\n",
    "    RoleName=role_name,\n",
    "    AssumeRolePolicyDocument=json.dumps(assume_role_policy_document)\n",
    ")\n",
    "\n",
    "policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"s3:*\",\n",
    "            \"Resource\": f\"arn:aws:s3:::*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"ecr:*\",\n",
    "            \"Resource\": f\"arn:aws:ecr:us-east-1:*:repository/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "create_policy_response = iam.create_policy(\n",
    "    PolicyName=f\"{role_name}Policy\",\n",
    "    PolicyDocument=json.dumps(policy_document)\n",
    ")\n",
    "\n",
    "iam.attach_role_policy(\n",
    "    RoleName=role_name,\n",
    "    PolicyArn=create_policy_response['Policy']['Arn']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "exec_role_arn=create_role_response['Role']['Arn']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::444975673530:role/SageMakerExecutionRole'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run this cell only if you need to get exec_role_arn for already created role\n",
    "iam = boto3.client('iam')\n",
    "role_name = 'SageMakerExecutionRole'\n",
    "create_role_response = iam.get_role(RoleName=role_name)\n",
    "exec_role_arn=create_role_response['Role']['Arn']\n",
    "exec_role_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This creates the endpoint with the appropriate permissions.\n",
    "You can use this to deploy multiple endpoints as long as the model_name, endpoint_name and endpoint_config_name are unique.\n",
    "Need to create a pull-through cache of public.ecr.aws/y0d1u8z0/llmm-cpu-arm64:latest' in your private ECR repo for this to work.\n",
    "@see https://docs.aws.amazon.com/AmazonECR/latest/userguide/pull-through-cache.html\n",
    "\"\"\"\n",
    "\n",
    "import boto3\n",
    "\n",
    "model_name = 'llamacpp-arm64-c7g-x8'\n",
    "#Graviton\n",
    "#image = '444975673530.dkr.ecr.us-east-1.amazonaws.com/y0d1u8z0/y0d1u8z0/llmm-cpu-arm64:latest'  #private pull-through cache of public.ecr.aws/y0d1u8z0/llmm-cpu-arm64:latest\n",
    "#x86\n",
    "#image = '444975673530.dkr.ecr.us-east-1.amazonaws.com/y0d1u8z0/y0d1u8z0/llmm-cpu-amd64:latest'  #private pull-through cache of public.ecr.aws/y0d1u8z0/llmm-cpu-arm64:latest\n",
    "image='444975673530.dkr.ecr.us-east-1.amazonaws.com/model-image-repository'\n",
    "endpoint_config_name = 'sm-llama-arm-config-c7g-x8'\n",
    "endpoint_name = 'sm-llama-arm-c7g-x8'\n",
    "instance_type=\"ml.c7g.8xlarge\" # make sure you use correct instance types x86 or graviton \n",
    "\n",
    "client = boto3.client('sagemaker', region_name='us-east-1')\n",
    "response = client.create_model(\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer={\n",
    "        'Image': image,\n",
    "        'Mode': 'SingleModel',\n",
    "    },\n",
    "    ExecutionRoleArn=exec_role_arn\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "response = client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'VariantName': 'default',\n",
    "            'ModelName': model_name,\n",
    "            'InitialInstanceCount': 1,\n",
    "            'InstanceType': instance_type,\n",
    "            'InitialVariantWeight': 1.0,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointArn': 'arn:aws:sagemaker:us-east-1:444975673530:endpoint/sm-llama-arm-c5-x9',\n",
       " 'ResponseMetadata': {'RequestId': 'e854ab6c-214e-475d-896c-5f056aa242fc',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'e854ab6c-214e-475d-896c-5f056aa242fc',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '86',\n",
       "   'date': 'Tue, 05 Dec 2023 16:27:31 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we define the functionality to interact with endpoint. First we need to configure it to load the GGUF or GGML models and then we can do the inference\n",
    "\"\"\"\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime', region_name='us-east-1')\n",
    "#endpoint_name='llm-cpu-llama-2-7b-chat-Endpoint'\n",
    "\n",
    "def invoke_sagemaker_endpoint(endpoint_name, llama_args):\n",
    "    payload = {\n",
    "        'inference': True,\n",
    "        'configure': False,\n",
    "        'args': llama_args\n",
    "    }\n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(llama_args),\n",
    "        ContentType='application/json',\n",
    "    )\n",
    "    response_body = json.loads(response['Body'].read().decode())\n",
    "    return response_body\n",
    "\n",
    "def invoke_sagemaker_streaming_endpoint(endpoint_name, payload):\n",
    "    response = sagemaker_runtime.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType='application/json',\n",
    "    )    \n",
    "    event_stream = response['Body']\n",
    "    for line in event_stream:\n",
    "        try:\n",
    "            itm = json.loads(line['PayloadPart']['Bytes'].decode(\"utf-8\")[6:])\n",
    "            print(itm[\"choices\"][0][\"text\"], end='')\n",
    "        except:\n",
    "            print(line['PayloadPart']['Bytes'].decode(\"utf-8\")[6:])\n",
    "\n",
    "\n",
    "def configure_sagemaker_endpoint(endpoint_name, llama_model_args):\n",
    "    payload = {\n",
    "        'configure': True,\n",
    "        'inference': False,\n",
    "        'args': llama_model_args\n",
    "    }\n",
    "\n",
    "    payload = {\"configure\": {\"bucket\":\"modeldownloadstack-modelbucketc6ceab13-1qgdn2wkgqda3\",\"key\":\"llama-2-7b-chat.Q4_K_M.gguf\"}}\n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType='application/json',\n",
    "    )\n",
    "    #response_body = json.loads(response['Body'].read().decode())\n",
    "    response_body = response['Body'].read().decode()\n",
    "    return response_body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Wait until your model is InService.\n",
    "\n",
    "This is to configure the model\n",
    "overwrite 'bucket' and 'key' with your path to the model file.\n",
    "set 'n_threads': NUMBER_OF_VPCUS - 1 to use all available VPCUS.\n",
    "Execute this cell each time you want to load a new model into the endpoint without having to redeploy anything. \n",
    "Loading model from S3 usualy takes 20-30 seconds but depends on loading speed from S3.\n",
    "\"\"\"\n",
    "\n",
    "llama_model_args = {\n",
    "    \"bucket\":\"alexvt-adx-emr-eks\",\n",
    "    \"key\":\"llama-2-7b-arguments.Q4_K_M.gguf\", \n",
    "    \"n_ctx\": 1024,\n",
    "    \"n_parts\": -1,\n",
    "    \"n_gpu_layers\": 0,\n",
    "    \"seed\": 1411,\n",
    "    \"f16_kv\": True,\n",
    "    \"logits_all\": False,\n",
    "    \"vocab_only\": False,\n",
    "    \"use_mmap\": True,\n",
    "    \"use_mlock\": False,\n",
    "    \"embedding\": False,\n",
    "    \"n_threads\": None,\n",
    "    \"n_batch\": 512,\n",
    "    \"last_n_tokens_size\": 64,\n",
    "    \"lora_base\": None,\n",
    "    \"lora_path\": None,\n",
    "    \"low_vram\": False,\n",
    "    \"tensor_split\": None,\n",
    "    \"rope_freq_base\": 10000,\n",
    "    \"rope_freq_scale\": 1,\n",
    "    \"verbose\": False,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "configuration = configure_sagemaker_endpoint(endpoint_name,llama_model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration\n",
    "#endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'choices': [{'finish_reason': 'stop',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'text': '\\nA. Determine the average number of documents per shard and calculate the ideal shard size based on that number.\\nB. Calculate the total amount of memory required for the index and divide it by the number of shards to determine the optimal shard size.\\nC. Monitor the performance of your search cluster and adjust the shard size as needed to maintain optimal performance.\\nD. Use a formula that takes into account the number of documents, the average document size, and the desired level of parallelism to calculate the ideal shard size.'}],\n",
       " 'created': 1701884722,\n",
       " 'id': 'cmpl',\n",
       " 'model': 'LLaMA_CPP',\n",
       " 'object': 'text_completion',\n",
       " 'truncated': False,\n",
       " 'usage': {'completion_tokens': 116, 'prompt_tokens': 26, 'total_tokens': 142}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Execute this cell each time you do the inference. Use the prompt format specific to the model you loaded.  \n",
    "\"\"\"\n",
    "#\"max_tokens\": 800,\n",
    "\n",
    "llama_args = {\n",
    "    \"prompt\": \"Give concise answer to the question. Qiestion: How to define optimal shard size in Amazon Opensearch?\",\n",
    "    \"max_tokens\": 128,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.5\n",
    "}\n",
    "\n",
    "inference = invoke_sagemaker_endpoint(endpoint_name,llama_args)\n",
    "inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
