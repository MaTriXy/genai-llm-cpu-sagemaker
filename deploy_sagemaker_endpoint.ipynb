{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This creates the IAM role for SageMaker  with the appropriate permissions. Make sure you have the bucket created first.\n",
    "You normally need to run it only once when setting things up\n",
    "\"\"\"\n",
    "\n",
    "import boto3\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "\n",
    "bucket_name = 'alexvt-adx-emr-eks'  # replace with your bucket name\n",
    "role_name = 'SageMakerExecutionRole'  # replace with your preferred role name\n",
    "\n",
    "assume_role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"sagemaker.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "create_role_response = iam.create_role(\n",
    "    RoleName=role_name,\n",
    "    AssumeRolePolicyDocument=json.dumps(assume_role_policy_document)\n",
    ")\n",
    "\n",
    "policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"s3:*\",\n",
    "            \"Resource\": f\"arn:aws:s3:::{bucket_name}/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "create_policy_response = iam.create_policy(\n",
    "    PolicyName=f\"{role_name}Policy\",\n",
    "    PolicyDocument=json.dumps(policy_document)\n",
    ")\n",
    "\n",
    "iam.attach_role_policy(\n",
    "    RoleName=role_name,\n",
    "    PolicyArn=create_policy_response['Policy']['Arn']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "exec_role_arn=create_role_response['Role']['Arn']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell only if you need to get exec_role_arn for already created role\n",
    "iam = boto3.client('iam')\n",
    "role_name = 'SageMakerExecutionRole'\n",
    "create_role_response = iam.get_role(RoleName=role_name)\n",
    "exec_role_arn=create_role_response['Role']['Arn']\n",
    "exec_role_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This creates the endpoint with the appropriate permissions.\n",
    "You can use this to deploy multiple endpoints as long as the model_name, endpoint_name and endpoint_config_name are unique.\n",
    "Need to create a pull-through cache of public.ecr.aws/y0d1u8z0/llmm-cpu-arm64:latest' in your private ECR repo for this to work.\n",
    "@see https://docs.aws.amazon.com/AmazonECR/latest/userguide/pull-through-cache.html\n",
    "\"\"\"\n",
    "\n",
    "model_name = 'llamacpp-arm64-c5-x9'\n",
    "#Graviton\n",
    "#image = '444975673530.dkr.ecr.us-east-1.amazonaws.com/y0d1u8z0/y0d1u8z0/llmm-cpu-arm64:latest'  #private pull-through cache of public.ecr.aws/y0d1u8z0/llmm-cpu-arm64:latest\n",
    "#x86\n",
    "image = '444975673530.dkr.ecr.us-east-1.amazonaws.com/y0d1u8z0/y0d1u8z0/llmm-cpu-amd64:latest'  #private pull-through cache of public.ecr.aws/y0d1u8z0/llmm-cpu-arm64:latest\n",
    "\n",
    "endpoint_config_name = 'sm-llama-arm-config-c5-x9'\n",
    "endpoint_name = 'sm-llama-arm-c5-x9'\n",
    "instance_type=\"ml.c5.9xlarge\" # make sure you use correct instance types x86 or graviton \n",
    "\n",
    "client = boto3.client('sagemaker', region_name='us-east-1')\n",
    "response = client.create_model(\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer={\n",
    "        'Image': image,\n",
    "        'Mode': 'SingleModel',\n",
    "    },\n",
    "    ExecutionRoleArn=exec_role_arn\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "response = client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'VariantName': 'default',\n",
    "            'ModelName': model_name,\n",
    "            'InitialInstanceCount': 1,\n",
    "            'InstanceType': instance_type,\n",
    "            'InitialVariantWeight': 1.0,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we define the functionality to interact with endpoint. First we need to configure it to load the GGUF or GGML models and then we can do the inference\n",
    "\"\"\"\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime', region_name='us-east-1')\n",
    "\n",
    "\n",
    "def invoke_sagemaker_endpoint(endpoint_name, llama_args):\n",
    "    payload = {\n",
    "        'inference': True,\n",
    "        'configure': False,\n",
    "        'args': llama_args\n",
    "    }\n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType='application/json',\n",
    "    )\n",
    "    response_body = json.loads(response['Body'].read().decode())\n",
    "    return response_body\n",
    "\n",
    "def configure_sagemaker_endpoint(endpoint_name, llama_model_args):\n",
    "    payload = {\n",
    "        'configure': True,\n",
    "        'inference': False,\n",
    "        'args': llama_model_args\n",
    "    }\n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType='application/json',\n",
    "    )\n",
    "    response_body = json.loads(response['Body'].read().decode())\n",
    "    return response_body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Wait until your model is InService.\n",
    "\n",
    "This is to configure the model\n",
    "overwrite 'bucket' and 'key' with your path to the model file.\n",
    "set 'n_threads': NUMBER_OF_VPCUS - 1 to use all available VPCUS.\n",
    "Execute this cell each time you want to load a new model into the endpoint without having to redeploy anything. \n",
    "Loading model from S3 usualy takes 20-30 seconds but depends on loading speed from S3.\n",
    "\"\"\"\n",
    "\n",
    "llama_model_args = {\n",
    "    \"bucket\":\"alexvt-adx-emr-eks\",\n",
    "    \"key\":\"llama-2-7b-arguments.Q4_K_M.gguf\", \n",
    "    \"n_ctx\": 1024,\n",
    "    \"n_parts\": -1,\n",
    "    \"n_gpu_layers\": 0,\n",
    "    \"seed\": 1411,\n",
    "    \"f16_kv\": True,\n",
    "    \"logits_all\": False,\n",
    "    \"vocab_only\": False,\n",
    "    \"use_mmap\": True,\n",
    "    \"use_mlock\": False,\n",
    "    \"embedding\": False,\n",
    "    \"n_threads\": None,\n",
    "    \"n_batch\": 512,\n",
    "    \"last_n_tokens_size\": 64,\n",
    "    \"lora_base\": None,\n",
    "    \"lora_path\": None,\n",
    "    \"low_vram\": False,\n",
    "    \"tensor_split\": None,\n",
    "    \"rope_freq_base\": 10000,\n",
    "    \"rope_freq_scale\": 1,\n",
    "    \"verbose\": False,\n",
    "}\n",
    "\n",
    "\n",
    "configuration = configure_sagemaker_endpoint(endpoint_name,llama_model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration\n",
    "#endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Execute this cell each time you do the inference. Use the prompt format specific to the model you loaded.  \n",
    "\"\"\"\n",
    "\n",
    "llama_args = {\n",
    "    \"prompt\": \"[INST]Give concise answer to the question. Qiestion: How to define optimal shard size in Amazon Opensearch?[/INST]\",\n",
    "    \"max_tokens\": 800,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.5,\n",
    "    \"logprobs\": None,\n",
    "    \"echo\": False,\n",
    "    \"stop\": [],\n",
    "    \"frequency_penalty\": 0,\n",
    "    \"presence_penalty\": 0,\n",
    "    \"repeat_penalty\": 1.1,\n",
    "    \"top_k\": 40,\n",
    "    \"stream\": False,\n",
    "    \"tfs_z\": 1,\n",
    "    \"mirostat_mode\": 2,\n",
    "    \"mirostat_tau\": 5,\n",
    "    \"mirostat_eta\": 0.1,\n",
    "    \"model\": None,\n",
    "}\n",
    "\n",
    "inference = invoke_sagemaker_endpoint(endpoint_name,llama_args)\n",
    "inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
