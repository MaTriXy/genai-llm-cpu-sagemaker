{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This creates the IAM role for SageMaker  with the appropriate permissions. Make sure you have the bucket created first.\n",
    "You normally need to run it only once when setting things up\n",
    "\"\"\"\n",
    "\n",
    "import boto3\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "\n",
    "bucket_name = 'alexvt-adx-emr-eks'  # replace with your bucket name\n",
    "role_name = 'SageMakerExecutionRole'  # replace with your preferred role name\n",
    "\n",
    "assume_role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"sagemaker.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "create_role_response = iam.create_role(\n",
    "    RoleName=role_name,\n",
    "    AssumeRolePolicyDocument=json.dumps(assume_role_policy_document)\n",
    ")\n",
    "\n",
    "policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"s3:*\",\n",
    "            \"Resource\": f\"arn:aws:s3:::{bucket_name}/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "create_policy_response = iam.create_policy(\n",
    "    PolicyName=f\"{role_name}Policy\",\n",
    "    PolicyDocument=json.dumps(policy_document)\n",
    ")\n",
    "\n",
    "iam.attach_role_policy(\n",
    "    RoleName=role_name,\n",
    "    PolicyArn=create_policy_response['Policy']['Arn']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "exec_role_arn=create_role_response['Role']['Arn']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This creates the endpoint with the appropriate permissions.\n",
    "You can use this to deploy multiple endpoints as long as the model_name, endpoint_name and endpoint_config_name are unique.\n",
    "Need to create a pull-through cache of public.ecr.aws/y0d1u8z0/llmm-cpu-arm64:latest' in your private ECR repo for this to work.\n",
    "@see https://docs.aws.amazon.com/AmazonECR/latest/userguide/pull-through-cache.html\n",
    "\"\"\"\n",
    "\n",
    "model_name = 'llamacpp-arm64-c7-x8'\n",
    "image = '444975673530.dkr.ecr.us-east-1.amazonaws.com/y0d1u8z0/y0d1u8z0/llmm-cpu-arm64:latest'  #private pull-through cache of public.ecr.aws/y0d1u8z0/llmm-cpu-arm64:latest\n",
    "endpoint_config_name = 'sm-llama-arm-config-c7-x8'\n",
    "endpoint_name = 'sm-llama-arm-c7-x8'\n",
    "instance_type=\"ml.c7g.8xlarge\" # make sure you use ARM64 instance types\n",
    "\n",
    "client = boto3.client('sagemaker', region_name='us-east-1')\n",
    "response = client.create_model(\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer={\n",
    "        'Image': image,\n",
    "        'Mode': 'SingleModel',\n",
    "    },\n",
    "    ExecutionRoleArn=exec_role_arn\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "response = client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'VariantName': 'default',\n",
    "            'ModelName': model_name,\n",
    "            'InitialInstanceCount': 1,\n",
    "            'InstanceType': instance_type,\n",
    "            'InitialVariantWeight': 1.0,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointArn': 'arn:aws:sagemaker:us-east-1:444975673530:endpoint/sm-llama-arm-c7-x16',\n",
       " 'ResponseMetadata': {'RequestId': '4a713c3a-1e96-48b0-b7ab-dd1a6b4aba16',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '4a713c3a-1e96-48b0-b7ab-dd1a6b4aba16',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '87',\n",
       "   'date': 'Sun, 08 Oct 2023 12:48:18 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we define the functionality to interact with endpoint. First we need to configure it to load the GGUF or GGML models and then we can do the inference\n",
    "\"\"\"\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime', region_name='us-east-1')\n",
    "\n",
    "\n",
    "def invoke_sagemaker_endpoint(endpoint_name, llama_args):\n",
    "    payload = {\n",
    "        'inference': True,\n",
    "        'configure': False,\n",
    "        'args': llama_args\n",
    "    }\n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType='application/json',\n",
    "    )\n",
    "    response_body = json.loads(response['Body'].read().decode())\n",
    "    return response_body\n",
    "\n",
    "def configure_sagemaker_endpoint(endpoint_name, llama_model_args):\n",
    "    payload = {\n",
    "        'configure': True,\n",
    "        'inference': False,\n",
    "        'args': llama_model_args\n",
    "    }\n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType='application/json',\n",
    "    )\n",
    "    response_body = json.loads(response['Body'].read().decode())\n",
    "    return response_body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Wait until your model is InService.\n",
    "\n",
    "This is to configure the model\n",
    "overwrite 'bucket' and 'key' with your path to the model file.\n",
    "set 'n_threads': NUMBER_OF_VPCUS - 1 to use all available VPCUS.\n",
    "Execute this cell each time you want to load a new model into the endpoint without having to redeploy anything. \n",
    "Loading model from S3 usualy takes 20-30 seconds but depends on loading speed from S3.\n",
    "\"\"\"\n",
    "\n",
    "llama_model_args = {\n",
    "    \"bucket\":\"alexvt-adx-emr-eks\",\n",
    "    \"key\":\"mistral-7b-instruct-v0.1.Q6_K.gguf\", \n",
    "    \"n_ctx\": 1024,\n",
    "    \"n_parts\": -1,\n",
    "    \"n_gpu_layers\": 0,\n",
    "    \"seed\": 1337,\n",
    "    \"f16_kv\": True,\n",
    "    \"logits_all\": False,\n",
    "    \"vocab_only\": False,\n",
    "    \"use_mmap\": True,\n",
    "    \"use_mlock\": False,\n",
    "    \"embedding\": False,\n",
    "    \"n_threads\": 31,\n",
    "    \"n_batch\": 512,\n",
    "    \"last_n_tokens_size\": 64,\n",
    "    \"lora_base\": None,\n",
    "    \"lora_path\": None,\n",
    "    \"low_vram\": False,\n",
    "    \"tensor_split\": None,\n",
    "    \"rope_freq_base\": 10000,\n",
    "    \"rope_freq_scale\": 1,\n",
    "    \"verbose\": False,\n",
    "}\n",
    "\n",
    "\n",
    "configuration = configure_sagemaker_endpoint(endpoint_name,llama_model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sm-llama-arm-c7-x16'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration\n",
    "#endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-ee7efb7b-4e5d-4b5f-a0d6-b74d29ded0b0',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1696769763,\n",
       " 'model': '/app/llm_model.bin',\n",
       " 'choices': [{'text': ' Amazon OpenSearch Service is a managed search and analytics service that makes it easy to set up, operate, and scale search solutions. It provides fast and reliable full-text search, hit highlighting, filtering, sorting, and aggregations capabilities. With Amazon OpenSearch Service, you can easily integrate search into your web or mobile applications, and you can also use it as a data source for other AWS services like Amazon Kinesis Data Analytics, Amazon Redshift, and Amazon QuickSight.',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 11, 'completion_tokens': 102, 'total_tokens': 113}}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Execute this cell each time you do the inference. Use the prompt format specific to the model you loaded.  \n",
    "\"\"\"\n",
    "\n",
    "llama_args = {\n",
    "    \"prompt\": \"What is Amazon Opensearch Service? A:\",\n",
    "    \"max_tokens\": 800,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.5,\n",
    "    \"logprobs\": None,\n",
    "    \"echo\": False,\n",
    "    \"stop\": [],\n",
    "    \"frequency_penalty\": 0,\n",
    "    \"presence_penalty\": 0,\n",
    "    \"repeat_penalty\": 1.1,\n",
    "    \"top_k\": 40,\n",
    "    \"stream\": False,\n",
    "    \"tfs_z\": 1,\n",
    "    \"mirostat_mode\": 0,\n",
    "    \"mirostat_tau\": 5,\n",
    "    \"mirostat_eta\": 0.1,\n",
    "    \"model\": None,\n",
    "}\n",
    "\n",
    "inference = invoke_sagemaker_endpoint(endpoint_name,llama_args)\n",
    "inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
