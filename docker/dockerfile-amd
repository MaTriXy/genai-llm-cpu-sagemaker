FROM ghcr.io/ggerganov/llama.cpp:full-cuda

# Sets dumping log messages directly to stream instead of buffering
ENV PYTHONUNBUFFERED=1
# Set MODELPATH environment variable
ENV MODELPATH=/app/llm_model.bin

ENV PATH=$PATH:/app

# The working directory in the Docker image
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    unzip \
    libcurl4-openssl-dev \
    python3 \
    python3-pip \
    python3-dev \
    git \
    psmisc

# Copy requirements.txt and install Python dependencies
COPY requirements.txt ./requirements.txt
#main application file
COPY main.py /app/
#sagemaker endpoints expects serve file to run the application
COPY serve /app/
COPY server.sh /app/

RUN chmod u+x serve
RUN chmod u+x server.sh

RUN pip3 install -r requirements.txt
RUN export PATH=/app:$PATH

ENTRYPOINT ["/bin/bash"]

# Expose port for the application to run on, has to be 8080
EXPOSE 8080
