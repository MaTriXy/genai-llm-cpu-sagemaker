{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This creates the IAM role for SageMaker  with the appropriate permissions. Make sure you have the bucket created first.\n",
    "You normally need to run it only once when setting things up\n",
    "\"\"\"\n",
    "\n",
    "import boto3\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "\n",
    "bucket_name = 'alexvt-adx-emr-eks'  # replace with your bucket name\n",
    "role_name = 'SageMakerExecutionRole'  # replace with your preferred role name\n",
    "\n",
    "assume_role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"sagemaker.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "create_role_response = iam.create_role(\n",
    "    RoleName=role_name,\n",
    "    AssumeRolePolicyDocument=json.dumps(assume_role_policy_document)\n",
    ")\n",
    "\n",
    "policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"s3:*\",\n",
    "            \"Resource\": f\"arn:aws:s3:::{bucket_name}/*\"\n",
    "        },\n",
    "        {\n",
    "            \"Action\": \"logs:PutLogEvents\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Resource\": \"<<ARN_NO>>\",\n",
    "            \"Sid\": \"Logs\"\n",
    "        },\n",
    "        {\n",
    "            \"Action\": [\n",
    "                \"logs:DescribeLogStreams\",\n",
    "                \"logs:CreateLogStream\",\n",
    "                \"logs:CreateLogGroup\"\n",
    "            ],\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Resource\": \"<<ARN_NO>>\",\n",
    "            \"Sid\": \"Logs2\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "create_policy_response = iam.create_policy(\n",
    "    PolicyName=f\"{role_name}Policy\",\n",
    "    PolicyDocument=json.dumps(policy_document)\n",
    ")\n",
    "\n",
    "iam.attach_role_policy(\n",
    "    RoleName=role_name,\n",
    "    PolicyArn=create_policy_response['Policy']['Arn']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "exec_role_arn=create_role_response['Role']['Arn']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::444975673530:role/SageMakerExecutionRole'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run this cell only if you need to get exec_role_arn for already created role\n",
    "import boto3\n",
    "iam = boto3.client('iam')\n",
    "role_name = 'SageMakerExecutionRole'\n",
    "create_role_response = iam.get_role(RoleName=role_name)\n",
    "exec_role_arn=create_role_response['Role']['Arn']\n",
    "exec_role_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This creates the endpoint with the appropriate permissions.\n",
    "You can use this to deploy multiple endpoints as long as the model_name, endpoint_name and endpoint_config_name are unique.\n",
    "Need to create a pull-through cache of public.ecr.aws/y0d1u8z0/llmm-cpu-arm64:latest' in your private ECR repo for this to work.\n",
    "@see https://docs.aws.amazon.com/AmazonECR/latest/userguide/pull-through-cache.html\n",
    "\"\"\"\n",
    "\n",
    "model_name = 'llamacpp-arm64-c7-x8-v00'\n",
    "#Graviton\n",
    "#image = '444975673530.dkr.ecr.us-east-1.amazonaws.com/y0d1u8z0/y0d1u8z0/llmm-cpu-arm64:latest'  #private pull-through cache of public.ecr.aws/y0d1u8z0/llmm-cpu-arm64:latest\n",
    "#x86\n",
    "image = '444975673530.dkr.ecr.us-east-1.amazonaws.com/y0d1u8z0/y0d1u8z0/llmm-cpu-arm64-full-v00:perf'  #private pull-through cache of public.ecr.aws/y0d1u8z0/llmm-cpu-arm64:latest\n",
    "\n",
    "endpoint_config_name = 'sm-llama-arm-config-c7-x8-v00'\n",
    "endpoint_name = 'sm-llama-arm-c7-x8-v00-stream'\n",
    "instance_type=\"ml.c7g.8xlarge\" # make sure you use correct instance types x86 or graviton \n",
    "\n",
    "client = boto3.client('sagemaker', region_name='us-east-1')\n",
    "response = client.create_model(\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer={\n",
    "        'Image': image,\n",
    "        'Mode': 'SingleModel',\n",
    "    },\n",
    "    ExecutionRoleArn=exec_role_arn\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "response = client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            'VariantName': 'default',\n",
    "            'ModelName': model_name,\n",
    "            'InitialInstanceCount': 1,\n",
    "            'InstanceType': instance_type,\n",
    "            'InitialVariantWeight': 1.0,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointArn': 'arn:aws:sagemaker:us-east-1:444975673530:endpoint/sm-llama-arm-c7-x8-v00-openblas',\n",
       " 'ResponseMetadata': {'RequestId': '564ebdd6-c1e8-4598-97da-14cf28c0cfaa',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '564ebdd6-c1e8-4598-97da-14cf28c0cfaa',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '99',\n",
       "   'date': 'Fri, 13 Oct 2023 19:33:34 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we define the functionality to interact with endpoint. First we need to configure it to load the GGUF or GGML models and then we can do the inference\n",
    "\"\"\"\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime', region_name='us-east-1')\n",
    "\n",
    "\n",
    "def invoke_sagemaker_endpoint(endpoint_name, llama_args):\n",
    "    payload = {\n",
    "        'inference': True,\n",
    "        'configure': False,\n",
    "        'args': llama_args\n",
    "    }\n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType='application/json',\n",
    "    )\n",
    "    response_body = json.loads(response['Body'].read().decode())\n",
    "    return response_body\n",
    "\n",
    "def configure_sagemaker_endpoint(endpoint_name, llama_model_args):\n",
    "    payload = {\n",
    "        'configure': True,\n",
    "        'inference': False,\n",
    "        'args': llama_model_args\n",
    "    }\n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType='application/json',\n",
    "    )\n",
    "    response_body = json.loads(response['Body'].read().decode())\n",
    "    return response_body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Wait until your model is InService.\n",
    "\n",
    "This is to configure the model\n",
    "overwrite 'bucket' and 'key' with your path to the model file.\n",
    "set 'n_threads': NUMBER_OF_VPCUS - 1 to use all available VPCUS.\n",
    "Execute this cell each time you want to load a new model into the endpoint without having to redeploy anything. \n",
    "Loading model from S3 usualy takes 20-30 seconds but depends on loading speed from S3.\n",
    "\"\"\"\n",
    "\n",
    "llama_model_args = {\n",
    "    \"bucket\":\"alexvt-adx-emr-eks\",\n",
    "    \"key\": \"llama-2-7b-chat.Q5_K_M.gguf\",\n",
    "    \"n_ctx\": 1024,\n",
    "    \"n_parts\": -1,\n",
    "    \"n_gpu_layers\": 0,\n",
    "    \"seed\": 1411,\n",
    "    \"f16_kv\": True,\n",
    "    \"logits_all\": False,\n",
    "    \"vocab_only\": False,\n",
    "    \"use_mmap\": False,\n",
    "    \"use_mlock\": True,\n",
    "    \"embedding\": False,\n",
    "    \"n_threads\": None,\n",
    "    \"n_batch\": 512,\n",
    "    \"last_n_tokens_size\": 64,\n",
    "    \"lora_base\": None,\n",
    "    \"lora_path\": None,\n",
    "    \"low_vram\": False,\n",
    "    \"tensor_split\": None,\n",
    "    \"rope_freq_base\": 10000,\n",
    "    \"rope_freq_scale\": 1,\n",
    "    \"verbose\": False,\n",
    "}\n",
    "\n",
    "\n",
    "configuration = configure_sagemaker_endpoint(endpoint_name,llama_model_args)\n",
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sm-llama-arm-c7-x8-v00-stream'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-5060b3c9-bffe-4e15-909e-451c9909798e',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1697010565,\n",
       " 'model': '/app/llm_model.bin',\n",
       " 'choices': [{'text': 'Based on the information provided in the >>CONTEXT<<, if you have 8 vCPUs, you can use a maximum of 6 shards per data node.',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 443, 'completion_tokens': 36, 'total_tokens': 479}}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Execute this cell each time you do the inference. Use the prompt format specific to the model you loaded.  \n",
    "\"\"\"\n",
    "template2 = \"\"\"Answer the question as truthfully as possible by using the provided informaiton in >>CONTEXT<<. If the answer is not contained within the >>CONTEXT<<, respond with \"I can't answer that\".\n",
    "\n",
    ">>CONTEXT<<:\n",
    "The overall goal of choosing a number of shards is to distribute an index evenly\\n                across all data nodes in the cluster. However, these shards shouldn't be too large\\n                or too numerous. A general guideline is to try to keep shard size between\\n                10–30 GiB for workloads where search latency is a key performance objective,\\n                and 30–50 GiB for write-heavy workloads such as log analytics.\\n\\nLarge shards can make it difficult for OpenSearch to recover from failure, but\\n                because each shard uses some amount of CPU and memory, having too many small shards\\n                can cause performance issues and out of memory errors. In other words, shards should\\n                be small enough that the underlying OpenSearch Service instance can handle them, but not so small\\n                that they place needless strain on the hardware. Shard to CPU ratio – When a shard is\\n                involved in an indexing or search request, it uses a vCPU to process the request. As\\n                a best practice, use an initial scale point of 1.5 vCPU per shard. If your instance\\n                type has 8 vCPUs, set your data node count so that each node has no more than six\\n                shards. Note that this is an approximation. Be sure to test your workload and scale\\n                your cluster accordingly.\\n\\nFor storage volume, shard size, and instance type recommendations, see the\\n                following resources:\\n\\nSizing Amazon OpenSearch Service domains\\n\\nPetabyte scale in Amazon OpenSearch Service\\n\\nAvoid storage skew\n",
    "\n",
    ">>QUESTION<<:\n",
    "if i have 8vcpus, how many shards i can use?\n",
    "\n",
    ">>ANSWER<<:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "llama_args = {\n",
    "    \"prompt\": template2,\n",
    "    \"max_tokens\": 1000,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.5,\n",
    "    \"logprobs\": None,\n",
    "    \"echo\": False,\n",
    "    \"stop\": [],\n",
    "    \"frequency_penalty\": 0,\n",
    "    \"presence_penalty\": 0,\n",
    "    \"repeat_penalty\": 1.1,\n",
    "    \"top_k\": 40,\n",
    "    \"stream\": False,\n",
    "    \"tfs_z\": 1,\n",
    "    \"mirostat_mode\": 0,\n",
    "    \"mirostat_tau\": 5,\n",
    "    \"mirostat_eta\": 0.1,\n",
    "    \"model\": None,\n",
    "}\n",
    "\n",
    "inference = invoke_sagemaker_endpoint(endpoint_name,llama_args)\n",
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine the shard and data node counts for an OpenSearch cluster, follow these steps:\n",
      "\n",
      "1. Analyze the data distribution: Determine how evenly distributed your data is across your indexes. If your data is heavily skewed towards one or a few indexes, consider creating additional shards for those indexes to balance the load.\n",
      "2. Consider shard size: The size of each shard can also impact performance. For example, if you have 5 GiB of data in an index, using a single shard may be more efficient than spreading it across multiple shards.\n",
      "3. Choose a shard count that is an even multiple of the data node count: This helps ensure that shards are evenly distributed across data nodes and prevents hot nodes. Refer to the best practices section for examples of appropriate shard counts based on data node counts.\n",
      "4. Determine the number of replicas: Replicas are used for fault tolerance and can improve search performance. Consider the importance of maintaining data consistency and the trade-off between availability and consistency when deciding on the number of replicas.\n",
      "5. Calculate the total number of shards and replicas: Multiply the shard count by the number of replicas to get the total number of shards and replicas in your cluster.\n",
      "6. Monitor and adjust as needed: Continuously monitor the performance of your cluster and make adjustments as needed, such as adding more shards or replicas or changing the shard size.\n",
      "\n",
      "By following these steps, you can determine an appropriate shard and data node count for your OpenSearch cluster that balances performance, scalability, and fault tolerance."
     ]
    }
   ],
   "source": [
    "#use with SM-compatible endpoint format\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime', region_name='us-east-1')\n",
    "\n",
    "def invoke_sagemaker_streaming_endpoint(endpoint_name, payload):\n",
    "    response = sagemaker_runtime.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType='application/json',\n",
    "    )    \n",
    "    event_stream = response['Body']\n",
    "    for line in event_stream:\n",
    "        print(line['PayloadPart']['Bytes'].decode(\"utf-8\"), end='')\n",
    "\n",
    "def invoke_sagemaker_endpoint(endpoint_name, payload):\n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType='application/json',\n",
    "    )\n",
    "    response_body = json.loads(response['Body'].read().decode())\n",
    "    return response_body\n",
    "\n",
    "template2 = \"\"\"Answer the question as truthfully as possible by using the provided informaiton in >>CONTEXT<<.\n",
    ">>CONTEXT<<:\n",
    "For example, for an index with five primary shards and one replica, each indexing\n",
    "            request touches 10 shards. In contrast, search requests are sent to n shards, where n  is\n",
    "            the number of primary shards. For an index with five primary shards and one replica,\n",
    "            each search query touches five shards (primary or replica) from that index.\n",
    "\n",
    "Determine shard and data node counts\n",
    "\n",
    "Use the following best practices to determine shard and data node counts for your\n",
    "                domain.\n",
    "\n",
    "Shard count – The distribution of shards\n",
    "                to data nodes has a large impact on a domain’s performance. When you have indexes\n",
    "                with multiple shards, try to make the shard count an even multiple of the data node\n",
    "                count. This helps to ensure that shards are evenly distributed across data nodes,\n",
    "                and prevents hot nodes. For example, if you have 12 primary shards, your data node\n",
    "                count should be 2, 3, 4, 6, or 12. However, shard count is secondary to shard\n",
    "                size—if you have 5 GiB of data, you should still use a single shard.\n",
    "\n",
    ">>QUESTION<<:\n",
    "Gvie instructions on how to determine shard and data node counts for OpenSearch?\n",
    "\n",
    ">>Answer<<:\n",
    "\"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": template2,\n",
    "    \"parameters\":{\n",
    "        \"max_new_tokens\": 600,\n",
    "        \"top_k\": 300,\n",
    "        \"top_p\": 0.95,\n",
    "        \"temperature\": 0.5,\n",
    "        \"stream\": True\n",
    "    }\n",
    "}\n",
    "inference = invoke_sagemaker_streaming_endpoint(endpoint_name,payload)\n",
    "#inference = invoke_sagemaker_endpoint(endpoint_name,payload)\n",
    "\n",
    "inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
